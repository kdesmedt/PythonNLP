{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOX/Gesic2p+L8Et0UPFMDT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EPdpx8NFPs2o"},"source":["# Word tokenization and frequencies with NLTK\n","\n","by Koenraad De Smedt at UiB"]},{"cell_type":"markdown","metadata":{"id":"wk_xz9xPZNpc"},"source":["---\n","\n","This notebook will introduce [NLTK](https://www.nltk.org/), the Natural Language ToolKit. This notebook will show how to do the following with this toolkit:\n","\n","1.  How to *word-tokenize* a text, i.e. make a list of tokens (words but also punctuation) obtained from a text string.\n","2.  How to compute the *vocabulary*, also called the *types*, i.e. the set of unique tokens.\n","3.  How to make a *frequency distribution*, i.e. a counter of token occurrences.\n","\n","In later notebooks, these techniques will be applied to larger texts read from the Web.\n","\n","For those who want to know more on NLTK, there is an [online book](https://www.nltk.org/book/).\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"OvY_xhTaO-A8"},"source":["The NLTK module provides several useful functions for manipulating text. See the [documentation](https://www.nltk.org/) if you want to know more."]},{"cell_type":"code","metadata":{"id":"3c3cyDgjN_t1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539940400,"user_tz":-60,"elapsed":2386,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"6c987a4b-8d43-4056-be82-a3642aca7544"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize, FreqDist"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}]},{"cell_type":"markdown","source":["Let's use Shakespeare's sonnet 141 as an example text."],"metadata":{"id":"VDqhINWeTi1H"}},{"cell_type":"code","source":["sonnet = '''In faith I do not love thee with mine eyes,\n","For they in thee a thousand errors note;\n","But 'tis my heart that loves what they despise,\n","Who, in despite of view, is pleased to dote.\n","Nor are mine ears with thy tongue's tune delighted;\n","Nor tender feeling, to base touches prone,\n","Nor taste, nor smell, desire to be invited\n","To any sensual feast with thee alone:\n","But my five wits nor my five senses can\n","Dissuade one foolish heart from serving thee,\n","Who leaves unswayed the likeness of a man,\n","Thy proud heart's slave and vassal wretch to be:\n","   Only my plague thus far I count my gain,\n","   That she that makes me sin awards me pain.'''"],"metadata":{"id":"UWaS6M5V7MSj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UbshcHjaOOQE"},"source":["## Tokenization\n","\n","NLTK provides the function `word_tokenize` which extracts all tokens and returns a list. This tokenizer is somewhat more sophisticated than the simple tokenizer from the previous notebook. Hyphenated words are kept together. Punctuation is split off and tokens for punctuation are included in the list. Notice how *'tis* and *heart's* are tokenized."]},{"cell_type":"code","metadata":{"id":"6dpX9ujKOZkI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539968422,"user_tz":-60,"elapsed":454,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"713185eb-ee3d-47b6-8293-7c0ece362eaf"},"source":["tokens = word_tokenize(sonnet)\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['In', 'faith', 'I', 'do', 'not', 'love', 'thee', 'with', 'mine', 'eyes', ',', 'For', 'they', 'in', 'thee', 'a', 'thousand', 'errors', 'note', ';', 'But', \"'t\", 'is', 'my', 'heart', 'that', 'loves', 'what', 'they', 'despise', ',', 'Who', ',', 'in', 'despite', 'of', 'view', ',', 'is', 'pleased', 'to', 'dote', '.', 'Nor', 'are', 'mine', 'ears', 'with', 'thy', 'tongue', \"'s\", 'tune', 'delighted', ';', 'Nor', 'tender', 'feeling', ',', 'to', 'base', 'touches', 'prone', ',', 'Nor', 'taste', ',', 'nor', 'smell', ',', 'desire', 'to', 'be', 'invited', 'To', 'any', 'sensual', 'feast', 'with', 'thee', 'alone', ':', 'But', 'my', 'five', 'wits', 'nor', 'my', 'five', 'senses', 'can', 'Dissuade', 'one', 'foolish', 'heart', 'from', 'serving', 'thee', ',', 'Who', 'leaves', 'unswayed', 'the', 'likeness', 'of', 'a', 'man', ',', 'Thy', 'proud', 'heart', \"'s\", 'slave', 'and', 'vassal', 'wretch', 'to', 'be', ':', 'Only', 'my', 'plague', 'thus', 'far', 'I', 'count', 'my', 'gain', ',', 'That', 'she', 'that', 'makes', 'me', 'sin', 'awards', 'me', 'pain', '.']\n"]}]},{"cell_type":"markdown","metadata":{"id":"9jatenuHPz3o"},"source":["Use `casefold` if you want to convert everything to lowercase. This may have advantages and disadvantages."]},{"cell_type":"code","metadata":{"id":"RMv9LlEIOw4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539972818,"user_tz":-60,"elapsed":332,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"e9b014ac-f044-4fdf-946c-7558e991913d"},"source":["tokens = word_tokenize(sonnet.casefold())\n","print(tokens)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['in', 'faith', 'i', 'do', 'not', 'love', 'thee', 'with', 'mine', 'eyes', ',', 'for', 'they', 'in', 'thee', 'a', 'thousand', 'errors', 'note', ';', 'but', \"'t\", 'is', 'my', 'heart', 'that', 'loves', 'what', 'they', 'despise', ',', 'who', ',', 'in', 'despite', 'of', 'view', ',', 'is', 'pleased', 'to', 'dote', '.', 'nor', 'are', 'mine', 'ears', 'with', 'thy', 'tongue', \"'s\", 'tune', 'delighted', ';', 'nor', 'tender', 'feeling', ',', 'to', 'base', 'touches', 'prone', ',', 'nor', 'taste', ',', 'nor', 'smell', ',', 'desire', 'to', 'be', 'invited', 'to', 'any', 'sensual', 'feast', 'with', 'thee', 'alone', ':', 'but', 'my', 'five', 'wits', 'nor', 'my', 'five', 'senses', 'can', 'dissuade', 'one', 'foolish', 'heart', 'from', 'serving', 'thee', ',', 'who', 'leaves', 'unswayed', 'the', 'likeness', 'of', 'a', 'man', ',', 'thy', 'proud', 'heart', \"'s\", 'slave', 'and', 'vassal', 'wretch', 'to', 'be', ':', 'only', 'my', 'plague', 'thus', 'far', 'i', 'count', 'my', 'gain', ',', 'that', 'she', 'that', 'makes', 'me', 'sin', 'awards', 'me', 'pain', '.']\n"]}]},{"cell_type":"markdown","metadata":{"id":"zzcDQsvXS9S4"},"source":["Make the vocabulary, i.e. the word types, by computing the set of unique tokens in the text."]},{"cell_type":"code","metadata":{"id":"Qz6AjoqoOcYM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539975279,"user_tz":-60,"elapsed":245,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"0d6cdfdf-94b5-4bb4-93d4-8b9f3c30b963"},"source":["vocab = set(tokens)\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'to', 'man', 'pleased', 'any', 'far', 'base', 'the', 'prone', 'with', 'dissuade', 'sensual', 'ears', 'despite', 'thee', 'only', 'be', 'faith', 'they', 'i', 'one', 'feeling', 'gain', 'and', 'serving', 'wits', 'alone', 'is', 'are', 'love', \"'s\", 'wretch', 'thus', 'nor', 'from', 'heart', 'unswayed', 'mine', 'view', '.', 'in', 'taste', ';', 'she', 'desire', 'feast', 'that', 'vassal', 'tender', 'touches', 'slave', 'likeness', 'note', 'do', 'five', 'eyes', 'leaves', 'awards', 'plague', 'pain', 'for', 'smell', 'dote', 'not', ',', 'invited', 'thy', 'can', 'foolish', 'proud', 'tongue', 'what', 'delighted', 'thousand', 'errors', 'sin', 'a', ':', 'my', 'of', 'me', 'but', 'tune', 'despise', 'makes', \"'t\", 'loves', 'senses', 'who', 'count'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"WQYb8dWFhdIX"},"source":["Print the length of the text and the length of the vocabulary."]},{"cell_type":"code","metadata":{"id":"qk52LiAOhirx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539977868,"user_tz":-60,"elapsed":373,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"fc95f662-47d8-4d8d-f716-000d4f655b84"},"source":["print(len(tokens), len(vocab))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["138 89\n"]}]},{"cell_type":"markdown","metadata":{"id":"WA3iBcH5-vmx"},"source":["Make a list of types with more than five characters."]},{"cell_type":"code","metadata":{"id":"GePsryfNHX-4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539979958,"user_tz":-60,"elapsed":288,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"9b4a4d39-62f3-4775-8845-b476104aa786"},"source":["print([word for word in vocab if len(word) > 5])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['pleased', 'dissuade', 'sensual', 'despite', 'feeling', 'serving', 'wretch', 'unswayed', 'desire', 'vassal', 'tender', 'touches', 'likeness', 'leaves', 'awards', 'plague', 'invited', 'foolish', 'tongue', 'delighted', 'thousand', 'errors', 'despise', 'senses']\n"]}]},{"cell_type":"markdown","metadata":{"id":"bxK9tnlrpjSj"},"source":["## Distribution of word counts\n","\n","In computational and corpus linguistics, the term *frequency* is often used in different ways:\n","\n","*   *Absolute frequencies* are simply counts, that is, the number of times something occurs in a text or corpus.\n","*   *Relative frequencies* are proportions of the number of occurrences to a certain amount of text (such as the length of a given text, or a million words)\n","\n","The NLTK function `FreqDist` computes the distribution of tokens in a text in terms of absolute frequencies. It produces a kind of *counter*, which is a special *dict* in which each token is associated with its number of occurrences."]},{"cell_type":"code","metadata":{"id":"LBiMpZolpaiM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539988062,"user_tz":-60,"elapsed":240,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"b262621b-6653-4318-8a76-98bfb4a76a5c"},"source":["freq_dist = FreqDist(tokens)\n","freq_dist"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({',': 11, 'my': 5, 'to': 5, 'nor': 5, 'thee': 4, 'in': 3, 'with': 3, 'heart': 3, 'that': 3, 'i': 2, ...})"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"uv4070vDprKx"},"source":["We can find the counts of a token by using the token as a key."]},{"cell_type":"code","metadata":{"id":"VwU1s07QpxvJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709539991728,"user_tz":-60,"elapsed":240,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"bbbfe524-0953-46d0-bac5-59e043b83f7b"},"source":["freq_dist['heart']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"lWpizjEvS2Lo"},"source":["Sort the counts with the most common first. This produces a list of tuples."]},{"cell_type":"code","metadata":{"id":"BIYQvxPLTU-m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709540004323,"user_tz":-60,"elapsed":249,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"37c1afec-5a30-4e68-a166-54fd24d82cc2"},"source":["freq_list = freq_dist.most_common(10)\n","print(freq_list)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(',', 11), ('my', 5), ('to', 5), ('nor', 5), ('thee', 4), ('in', 3), ('with', 3), ('heart', 3), ('that', 3), ('i', 2)]\n"]}]},{"cell_type":"markdown","metadata":{"id":"D0ZFL53VgFaV"},"source":["Print the nine most common items in the list, with the counts."]},{"cell_type":"code","metadata":{"id":"4pzk5ULxgA7j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709540033151,"user_tz":-60,"elapsed":3,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"1712b09c-fbd7-4aa6-f0b3-a4fb3b3f9a6e"},"source":["for (item, freq) in freq_dist.most_common(10):\n","  print(freq, ':', item)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["11 : ,\n","5 : my\n","5 : to\n","5 : nor\n","4 : thee\n","3 : in\n","3 : with\n","3 : heart\n","3 : that\n","2 : i\n"]}]},{"cell_type":"markdown","source":["NLTK can also provide a list of stopwords."],"metadata":{"id":"hXYzNzs0jACJ"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stop_eng = stopwords.words('english')\n","print(stop_eng[:12])"],"metadata":{"id":"vnZdWumYbDeb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709540208771,"user_tz":-60,"elapsed":232,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"58562987-8fa5-46ca-9e4c-729bca9a9952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\"]\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["Compute the set of words in `vocab` which are not stopwords by using the minus sign `-` for set difference."],"metadata":{"id":"J5x114GCjGCG"}},{"cell_type":"code","source":["non_stop = vocab - set(stop_eng)\n","print(non_stop)\n","len(non_stop)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JS-NEoVsiHS1","executionInfo":{"status":"ok","timestamp":1709540299351,"user_tz":-60,"elapsed":240,"user":{"displayName":"Koenraad De Smedt","userId":"03116529981388996672"}},"outputId":"dff641e1-5ff3-4c12-d34b-d9a8b9b83269"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'slave', 'likeness', 'note', 'five', 'proud', 'tongue', 'gain', 'man', 'serving', 'wits', 'delighted', 'pleased', 'eyes', 'touches', 'leaves', 'thousand', 'errors', 'alone', 'awards', 'sin', 'plague', 'far', 'base', 'love', \"'s\", 'wretch', 'thus', ':', 'pain', 'heart', 'smell', 'prone', 'unswayed', 'mine', 'view', 'dissuade', '.', 'taste', 'dote', 'count', 'sensual', 'tune', 'ears', ';', 'despite', 'thee', 'desire', 'despise', ',', 'makes', 'feast', 'faith', 'invited', \"'t\", 'thy', 'loves', 'vassal', 'foolish', 'senses', 'one', 'tender', 'feeling'}\n"]},{"output_type":"execute_result","data":{"text/plain":["62"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["## Languages other than English\n","\n","NLTK supports some other languages, but the coverage of some forms, such as elisions, is not complete. See the following lines from [Kindertotenlieder](https://oxfordsong.org/song/kindertotenlieder)."],"metadata":{"id":"LFvJj6jzH5uL"}},{"cell_type":"code","source":["verses = '''Du mußt nicht die Nacht in dir verschränken,\n","Mußt sie ins ew’ge Licht versenken!\n","[...]\n","Was dir nur Augen sind in diesen Tagen:\n","In künft’gen Nächten sind es dir nur Sterne.'''\n","\n","gtokens = word_tokenize(verses, language='german')\n","print(gtokens)"],"metadata":{"id":"QgwAnUUmHMUW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZCYKL37_a1g"},"source":["### Exercises\n","\n","1.   Compute the lexical variation, i.e. the ratio of types to tokens, of the sonnet (or another text).\n","2.   Print the nine most common tokens with their counts, but also print its length on the same line.\n","3.   Instead of printing in the previous exercise, use a comprehension to make a list of triples containing the count, the item and the item's length.\n","4.   Print the five most common tokens with at least three characters. The easiest is to first use a comprehension that includes items with at least three characters, and then make a frequency distribution of that list.\n","5.   (optional) For some purposes, one wants a list containing only words, excluding tokens that consist of just punctuation marks. What needs to be done to keep only words?\n","> Tip: if you `import string`, you can use the variable `string.punctuation` which has all punctuation marks. So you can write a function that checks if all characters of a token are in `string.punctuation` or not. Then you can use that function in a comprehension over all tokens."]}]}