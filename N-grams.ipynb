{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP58GSswqJNFH7I1luHVI2e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9kbYAVzrCvAq"},"source":["# Word n-grams with zip and with NLTK\n","\n","by Koenraad De Smedt at UiB"]},{"cell_type":"markdown","metadata":{"id":"sX37WY35C5eH"},"source":["---\n","*N-grams* are consecutive parts of a text broken up into *n* tokens (words or letters), such as the following word *trigrams* (3-grams) for *Je pense, donc je suis* (a famous line by René Descartes).\n","\n","> ```\n","Je pense donc\n","pense donc je\n","donc je suis\n","```\n","\n","or, if we also consider punctuation to be tokens,\n","\n","> ```\n","Je pense ,\n","pense , donc\n",", donc je\n","donc je suis\n","je suis .\n","```\n","\n","N-grams shows words in their (limited) contexts. Computing all n-grams in a text or corpus can be useful for several NLP purposes, such as translation, error correction, finding collocations, document classification, etc. Imagine you want to correct a misspelled word and you have two possible corrections, one which occurs in an n-gram based on a large corpus, and one which does not occur in such a context, then you might prefer the word occurring in the n-gram.\n","\n","This notebook shows how we can compute n-grams as a list of tuples by using `zip`. Also the `ngrams` function in NLTK is demonstrated.\n","\n","---"]},{"cell_type":"markdown","source":["Let's start with list of tokens, such as the following famous quote from René Descartes. Also, suppose we want to make trigrams (3-grams), so we set `n` to `3`."],"metadata":{"id":"lWNGupnCmU_g"}},{"cell_type":"code","source":["tokens = ['Je', 'pense', 'donc', 'je', 'suis']\n","n = 3"],"metadata":{"id":"JcZSVdKpmC1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can make partial copies of this list starting at item 0, 1, 2, ... until we reach n.\n"],"metadata":{"id":"ObA3fJCb6ASr"}},{"cell_type":"code","metadata":{"id":"3_bBMWv66kla"},"source":["partlists = [tokens[i:] for i in range(n)]\n","partlists"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the previous result, read the lists vertically from left to right, and you see the trigrams appear. So how do we combine the first elements of each of these lists, then their second elements, etc.?\n","\n","```\n","Je        pense     donc\n","↓         ↓         ↓\n","pense     donc      je\n","↓         ↓         ↓\n","donc      je        suis\n","```"],"metadata":{"id":"JGXc3dtu7cji"}},{"cell_type":"markdown","metadata":{"id":"2M89O12r8jSL"},"source":["The solution is to use `zip`. If we unpack `partlists` and give the contained lists as arguments to `zip`, we get a list of tuples which are word n-grams – in this case, trigrams."]},{"cell_type":"code","metadata":{"id":"BzvkyoES7ITw"},"source":["tri = zip(*partlists)\n","[*tri]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We are now ready to define a function that operates on a list of tokens and produces n-gram tuples. It has an extra argument for *n* with a default of 3."],"metadata":{"id":"SkjWG8slNF18"}},{"cell_type":"code","metadata":{"id":"fUQshbBNCt1X"},"source":["def n_grams (seq, n=3):\n","  return zip(*[seq[i:] for i in range(n)])\n","\n","ng = n_grams(tokens)\n","ng"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unpack the zip items into a list if wanted."],"metadata":{"id":"_YzkrBBMtAFB"}},{"cell_type":"code","source":["[*ng]"],"metadata":{"id":"jCkoAAyvs8L9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Override the default in order to produce bigrams."],"metadata":{"id":"GzzChYCabaoH"}},{"cell_type":"code","source":["[*n_grams(tokens, n=2)]"],"metadata":{"id":"w4QKFHqYX-fV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ov6bDUM9ER6q"},"source":["---\n","## N-grams with NLTK\n","\n","Now that you understand how n-grams can be computed, let's look at NLTK which also provides a tool for n-grams. If we want to tokenize first, we need to import the tokenize module as well as the ngrams module."]},{"cell_type":"code","metadata":{"id":"RGFJJk7OEQ-1"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize\n","from nltk.util import ngrams"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-pjrr9MZYcVF"},"source":["In the `ngrams` function provided by NLTK, the second argument is obligatory; it has no default. The result is also a zip of tuples."]},{"cell_type":"code","metadata":{"id":"FQjDop43Yf11"},"source":["cogito = 'Je pense, donc je suis fatigué...'\n","ng = ngrams(word_tokenize(cogito), n=3)\n","ng"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Unpack the zip items into a list if wanted. This result is similar to the result above."],"metadata":{"id":"DEB5EYZKtm4P"}},{"cell_type":"code","source":["[*ng]"],"metadata":{"id":"AEO0MzfZtenq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8HHKgIQhkNpj"},"source":["###User interactions to set parameters (optional)\n","\n","Google Colab offers user interactions to set parameters. These are indicated as special comments with the `#@` characters. The following illustrates the use of a *slider* to choose the length of the n-grams. See the [forms example](https://colab.research.google.com/notebooks/forms.ipynb) for more possibilities. This may not work outside of Google Colab, but [IPywidgets](https://towardsdatascience.com/interactive-controls-for-jupyter-notebooks-f5c94829aee6) offers something similar."]},{"cell_type":"code","metadata":{"id":"bkLFPwY2jvvC"},"source":["#@title Choose the length of the n-grams\n","N_gram_length = 2 #@param {type:\"slider\", min:2, max:5, step:1}\n","ng2 = ngrams(word_tokenize(cogito), n=N_gram_length)\n","[*ng2][:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"asfH5FN4FaHc"},"source":["### Exercises\n","\n","1.   Use a larger text, tokenize and compute the word bigrams.\n","2.   Compute the number of *different* bigrams in your result.\n","3.   Convert the n-gram tuples to strings with spaces, for instance, `'Je pense donc'`.\n","4.   Make a frequency list of the n-grams, for instance by using a Counter. Get the 5 most common n-grams.\n","5.   Compute *character* n-grams from a single text string, using `n_grams` or `ngrams`. Compare the result with the approach in the notebook on Ranges."]}]}