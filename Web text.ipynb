{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"colab":{"provenance":[{"file_id":"1DbF69sEbzOWEzyDl5ZUkrJSd0PGfc_vM","timestamp":1619513509057}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"637cc60d"},"source":["# Reading plain text from the web\n","\n","by Koenraad De Smedt at UiB"],"id":"637cc60d"},{"cell_type":"markdown","metadata":{"id":"lME03Clev2yx"},"source":["---\n","There is a lot of textual material on the web that can be read and processed.\n","The [CLARIN VLO](https://vlo.clarin.eu) is a searchable catalog of language resources in many formats. There is also much literature at [Project Gutenberg](https://gutenberg.org/).\n","\n","This notebook will deal with the simplest format, namely, plain Unicode text. You will learn the following:\n","\n","1.   Read a plain text from the web into a string\n","2.   Tokenize the text and compute the types and lexical variation.\n","3.   Read and process plain text, line by line, from the web\n","\n","---"],"id":"lME03Clev2yx"},{"cell_type":"markdown","source":["# 0. Getting some text\n","\n","We need to import the `requests` module that can send a request to a webpage based on its url."],"metadata":{"id":"yF_s9HMltE3D"},"id":"yF_s9HMltE3D"},{"cell_type":"code","metadata":{"id":"hHlJrSVxzkVt"},"source":["import requests"],"id":"hHlJrSVxzkVt","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If you search for *Utopia* on the CLARIN VLO, you will find a [metadata record for Thomas More's Utopia](https://vlo.clarin.eu/record/https_58__47__47_hdl.handle.net_47_20.500.14106_47_3220_64_format_61_cmdi?26). That page has nine linked resources, one of which is the following link to plain text at the Oxford Text Archive. You can open it in a new tab in the browser to check that it contains plain text."],"metadata":{"id":"t05hVoxHrDvp"},"id":"t05hVoxHrDvp"},{"cell_type":"code","source":["utopia_url = 'https://llds.ling-phil.ox.ac.uk/llds/xmlui/bitstream/handle/20.500.14106/3220/3220.txt?sequence=8'"],"metadata":{"id":"AHKgqkjuo6oH"},"id":"AHKgqkjuo6oH","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You may think of copying the whole text and pasting it into a string, but that would be inconvenient for various reasons. Instead, we will get the text directly from the webpage into Python."],"metadata":{"id":"TxGntlJSsvre"},"id":"TxGntlJSsvre"},{"cell_type":"markdown","metadata":{"id":"lX3xMtd2Aqie"},"source":["The function `requests.get` opens a webpage based on the url. There are several kinds of information in the response, but here we are only interested in getting the textual content as a Unicode string by means of `.text`.\n","In this example, we take only a 1000 characters because the whole text is too big to display here."],"id":"lX3xMtd2Aqie"},{"cell_type":"code","metadata":{"id":"UONEr_USJnsJ"},"source":["utopia_text = requests.get(utopia_url).text[:1000]\n","print(utopia_text)"],"id":"UONEr_USJnsJ","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L3ydjfB3q3Zv"},"source":["# 1. Computing tokens, types and distribution.\n","\n","Now that we have plain text, we can further process it. We import the `nltk` module, which provides some useful text manipulation and counting functions."],"id":"L3ydjfB3q3Zv"},{"cell_type":"code","metadata":{"id":"PoHSmzUkNOeM"},"source":["import nltk\n","nltk.download('punkt')\n","from nltk import word_tokenize, FreqDist"],"id":"PoHSmzUkNOeM","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YgquohDFX420"},"source":["Make a list of word tokens from the lowercased text."],"id":"YgquohDFX420"},{"cell_type":"code","metadata":{"id":"JvA9hcIWrU2l"},"source":["utopia_tokens = word_tokenize(utopia_text.lower())\n","print(utopia_tokens[:30])"],"id":"JvA9hcIWrU2l","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The set of types can also be called the vocabulary of the text."],"metadata":{"id":"Jaak2XJb8W8x"},"id":"Jaak2XJb8W8x"},{"cell_type":"code","source":["utopia_types = set(utopia_tokens)\n","print(utopia_types)"],"metadata":{"id":"75zklBR-8VOk"},"id":"75zklBR-8VOk","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"amU3kxVDsM3T"},"source":["We can compute the lexical variation by dividing the number of types by the number of tokens. The larger this number, the more varied use of words. The lower this number, the more repetition of words. For a very short text, this number doesn't mean all that much."],"id":"amU3kxVDsM3T"},{"cell_type":"code","metadata":{"id":"_FeSdAKCsLwt"},"source":["len(utopia_types) / len(utopia_tokens)"],"id":"_FeSdAKCsLwt","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aA0ynta7cqbL"},"source":["Let's define a function for lexical variation based on this proportion."],"id":"aA0ynta7cqbL"},{"cell_type":"code","metadata":{"id":"ST4VaNQFcIGQ"},"source":["def lexical_variation (text):\n","  tokens = word_tokenize(text.lower())\n","  types = set(tokens)\n","  return len(types) / len(tokens)\n","\n","lexical_variation(utopia_text)"],"id":"ST4VaNQFcIGQ","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make a distribution and get the count of a token."],"metadata":{"id":"oKloHdy53Tnd"},"id":"oKloHdy53Tnd"},{"cell_type":"code","source":["counts = FreqDist(utopia_tokens)\n","counts['prince'] # assume tokens are all lowercase"],"metadata":{"id":"rU8OY5dG_t3H"},"id":"rU8OY5dG_t3H","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Streaming line by line\n","\n","Instead of reading some or all characters of a webpage into a string, it is also possible to read and process *streamed* content line by line (if the text is divided in somewhat meaningful lines).\n","\n","What we get from `iter_lines` is an iterator, so that only as many lines are read as the program asks for by means of `next`. The code in the following cell reads and prints the first 20 lines only and also prints a line counter.\n","\n","By default, `iter_lines` produces raw strings (without newlines), so we need to tell it to decode each line into text."],"metadata":{"id":"Agwt6dOy9sfk"},"id":"Agwt6dOy9sfk"},{"cell_type":"code","source":["utopia_stream = requests.get(utopia_url, stream=True)\n","linestream = utopia_stream.iter_lines(decode_unicode=True)\n","for n in range(20):\n","  print(n, next(linestream))"],"metadata":{"id":"pE9W_3byxCjz"},"id":"pE9W_3byxCjz","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here is an alternative way of reading and printing 20 lines. We zip a range of numbers and a stream of lines. Again, this is very efficient because if the range is limited to 20, only 20 lines are read and zipped."],"metadata":{"id":"R1okFZMFxbT-"},"id":"R1okFZMFxbT-"},{"cell_type":"code","source":["utopia_stream = requests.get(utopia_url, stream=True)\n","for n, line in zip(range(20), utopia_stream.iter_lines(decode_unicode=True)):\n","  print(n, line)"],"metadata":{"id":"Wddpi1-kqiqg"},"id":"Wddpi1-kqiqg","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Suppose we want to read 20 lines and print only lines containing a double quote sign, we add a condition with `if`."],"metadata":{"id":"U569Pkj1okyK"},"id":"U569Pkj1okyK"},{"cell_type":"code","source":["utopia_stream = requests.get(utopia_url, stream=True)\n","for n, line in zip(range(20), utopia_stream.iter_lines(decode_unicode=True)):\n","  if '\"' in line:\n","    print(n, line)"],"metadata":{"id":"_yWp7lXypRfN"},"id":"_yWp7lXypRfN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The previous counts 20 lines read, not 20 lines written. Suppose we want to print 20 lines with double quotes, then we should use a counter that is increased only after we know we have a line that we want."],"metadata":{"id":"QtT_mbcOsfa0"},"id":"QtT_mbcOsfa0"},{"cell_type":"code","source":["utopia_stream = requests.get(utopia_url, stream=True)\n","line_iterator = utopia_stream.iter_lines(decode_unicode=True)\n","printed = 0\n","while printed < 20:\n","  line = next(line_iterator)\n","  if '\"' in line:\n","    print(printed, line)\n","    printed += 1"],"metadata":{"id":"xjDI9xBGtdd3"},"id":"xjDI9xBGtdd3","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z1Q3GxHUKcyA"},"source":["## Exercises\n","\n","1.  Read the full text of *Utopia* into Python. Do not print the whole text, because it is too long, but lowercase it, tokenize it and compute the lexical variation.\n","\n","2.  Compute the distribution of tokens in the full text (see the notebook on tokenization and frequencies with NLTK). Print the counts of *I*, *you*, *he* and *she*. Also, compute the relative frequency of these words per million words. Optionally, make a barplot with the counts.\n","\n","3.  Extend the code for reading lines with double quotes so that two counters are printed, one that counts the lines read and another that counts the lines printed.\n","\n","4.  Read [one of the plain texts of the taped diary](https://llds.ling-phil.ox.ac.uk/llds/xmlui/bitstream/handle/20.500.14106/0070/tape1-0070.txt?sequence=8) of [Patty Hearst](https://www.youtube.com/watch?v=kDHccwiT_0E) available at the Oxford Text Archive. Notice that the first two lines are metadata: they are not part of the actual content. There are at least two possible strategies:\n"," - Read two lines with `next` but ignore them, then read the remaining lines and join them with newline;\n"," - Read the whole file but delete the first two lines with a *regex*.\n","\n","5.  Find a large word list online with one word on each line. Iterate over its lines and print only the lines that are palindromes. Reuse the palindrome function from the earlier notebook about palindromes. The following are possible URLs for a large English word list. Alternatively, you can look for a list in another language.\n","\n"," *   http://wiki.puzzlers.org/pub/wordlists/unixdict.txt\n"," *   https://raw.githubusercontent.com/quinnj/Rosetta-Julia/master/unixdict.txt\n"," *   https://searchcode.com/codesearch/raw/29038705/\n","\n","6.  (optional) Suppose you need help in solving a crossword puzzle. Write a function `search_words` that iterates an online word list, as suggested above, and prints all lines matching a given regex. For instance, `search_words('^[db]a...$')` will look for five-letter words starting with *d* or *b* followed by *a*. You may want to limit the number of words that are printed because there could be many.\n","\n","7.  (optional) German has some very long words. Using a large word list for German (e.g. https://gist.githubusercontent.com/MarvinJWendt/2f4f4154b8ae218600eb091a5706b5f4/raw/36b70dd6be330aa61cd4d4cdfda6234dcb0b8784/wordlist-german.txt), iterate over its lines and select lines longer than 40 characters. Print each of those words or collect them in a list."],"id":"Z1Q3GxHUKcyA"},{"cell_type":"markdown","metadata":{"id":"DcTJuVxtahxS"},"source":["## Notes\n","\n","1.  See the [documentation of requests](https://docs.python-requests.org/en/master/user/quickstart/) if you need more possibilities to access webpages.\n","In case a webpage is encoded in anything else than UTF-8, then instead of `.text`, one can also use `.content.decode(encoding)`,  which gets the content and interprets that according to the given encoding, for instance `.content.decode('cp1252')`\n","\n","2.  In some versions of Python on MacOS, the use of the *requests* module may cause an error about certificates. If that is the case, you can activate the *Install Certificates.command* on MacOS. That command should be in the Python folder in your Applications folder."],"id":"DcTJuVxtahxS"}]}